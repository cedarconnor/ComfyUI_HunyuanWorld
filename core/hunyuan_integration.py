"""
Real HunyuanWorld-1.0 Model Integration - FLUX + LoRA ONLY
No fallbacks or procedural generation - only real AI inference
"""

import os
import sys
import torch
from typing import Dict, Any, Optional, Union
from pathlib import Path

# Add HunyuanWorld path to sys.path
def setup_hunyuan_path():
    """Add HunyuanWorld-1.0 to Python path"""
    current_dir = Path(__file__).parent.parent
    hunyuan_path = current_dir / "HunyuanWorld-1.0"
    
    if hunyuan_path.exists():
        sys.path.insert(0, str(hunyuan_path))
        print(f"[SUCCESS] Added HunyuanWorld path: {hunyuan_path}")
        return True
    else:
        print(f"[ERROR] HunyuanWorld-1.0 directory not found at: {hunyuan_path}")
        return False

# Setup path before imports
setup_hunyuan_path()

try:
    # Import HunyuanWorld components
    from hy3dworld import Text2PanoramaPipelines, Image2PanoramaPipelines
    from hy3dworld import LayerDecomposition, WorldComposer
    from hy3dworld.utils import Perspective, process_file
    HUNYUAN_AVAILABLE = True
    print("[SUCCESS] HunyuanWorld imports successful")
except ImportError as e:
    print(f"[WARNING] HunyuanWorld import failed: {e}")
    if "utils3d" in str(e):
        print("[INFO] Install utils3d manually: pip install git+https://github.com/EasternJournalist/utils3d.git")
    HUNYUAN_AVAILABLE = False

class HunyuanTextToPanoramaModel:
    """Real HunyuanWorld Text-to-Panorama Model Integration - FLUX + LoRA ONLY"""
    
    def __init__(self, model_path: str, device: str = "cuda", precision: str = "fp16"):
        self.model_path = model_path
        self.device = device
        self.precision = precision
        self.pipeline = None
        self.is_loaded = False
        
        if HUNYUAN_AVAILABLE:
            self._load_pipeline()
        else:
            print("[CRITICAL ERROR] HunyuanWorld not available")
            print("[FAILURE REASON] Cannot load real AI inference components")
            print("[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError("HunyuanWorld not available. Please ensure proper installation.")
    
    def _load_pipeline(self):
        """Load the FLUX + HunyuanWorld LoRA pipeline - NO FALLBACKS"""
        try:
            print(f"[INFO] Loading FLUX + HunyuanWorld pipeline...")
            
            # Create real FLUX pipeline with LoRA
            self.pipeline = self._create_flux_lora_pipeline()
            
            self.is_loaded = True
            print(f"[SUCCESS] FLUX + HunyuanWorld pipeline loaded successfully")
            
        except Exception as e:
            print(f"[CRITICAL ERROR] Failed to load FLUX + HunyuanWorld pipeline: {e}")
            print(f"[FAILURE REASON] Real AI inference required but failed")
            print(f"[NO FALLBACK] Procedural generation disabled")
            self.is_loaded = False
            raise RuntimeError(f"FLUX + HunyuanWorld pipeline loading failed: {e}")
    
    def generate_panorama(self, prompt: str, **kwargs):
        """Generate panorama using FLUX + HunyuanWorld LoRA ONLY"""
        if not self.is_loaded or not HUNYUAN_AVAILABLE:
            print("[CRITICAL ERROR] FLUX + HunyuanWorld models not loaded")
            print("[FAILURE REASON] Cannot generate without real AI inference")
            print("[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError("FLUX + HunyuanWorld models not loaded. Please ensure proper installation.")
        
        try:
            print(f"[INFO] Generating panorama with FLUX + HunyuanWorld LoRA: '{prompt}'")
            
            # Extract parameters
            height = kwargs.get('height', 960)
            width = kwargs.get('width', 1920)
            num_inference_steps = kwargs.get('num_inference_steps', 50)
            guidance_scale = kwargs.get('guidance_scale', 7.5)
            
            # Generate panorama using FLUX + LoRA - NO FALLBACKS
            result = self.pipeline(
                prompt=prompt,
                height=height,
                width=width,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
            )
            
            # Convert PIL Image to tensor
            if hasattr(result, 'images') and len(result.images) > 0:
                pil_image = result.images[0]
                # Convert PIL to tensor (H, W, C) format
                import numpy as np
                image_array = np.array(pil_image).astype(np.float32) / 255.0
                tensor = torch.from_numpy(image_array)
                
                print(f"[SUCCESS] FLUX + HunyuanWorld panorama generated: {tensor.shape}")
                return tensor
            else:
                print(f"[CRITICAL ERROR] No image generated by FLUX + HunyuanWorld pipeline")
                print(f"[FAILURE REASON] Pipeline executed but produced no output")
                raise RuntimeError("No image generated by FLUX + HunyuanWorld pipeline")
                
        except Exception as e:
            print(f"[CRITICAL ERROR] FLUX + HunyuanWorld panorama generation failed: {e}")
            print(f"[FAILURE REASON] Real AI inference required but failed")
            print(f"[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError(f"Panorama generation failed: {e}")
    
    def to(self, device):
        """Move model to device"""
        self.device = device
        if self.pipeline is not None:
            self.pipeline = self.pipeline.to(device)
        return self
    
    def cpu(self):
        """Move model to CPU"""
        return self.to("cpu")
    
    def _create_flux_lora_pipeline(self):
        """Create FLUX + HunyuanWorld LoRA pipeline - NO FALLBACKS"""
        try:
            # Try different import paths for ComfyUI compatibility
            try:
                from .working_flux_pipeline import create_working_flux_pipeline
                return create_working_flux_pipeline(self.model_path, self.device)
            except ImportError:
                import sys
                import os
                current_dir = os.path.dirname(__file__)
                sys.path.insert(0, current_dir)
                from working_flux_pipeline import create_working_flux_pipeline
                return create_working_flux_pipeline(self.model_path, self.device)
            
        except Exception as e:
            print(f"[CRITICAL ERROR] FLUX + LoRA pipeline creation failed: {e}")
            print(f"[FAILURE REASON] Cannot load FLUX + HunyuanWorld integration")
            print(f"[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError(f"FLUX + LoRA pipeline creation failed: {e}")

class HunyuanImageToPanoramaModel:
    """Real HunyuanWorld Image-to-Panorama Model Integration - FLUX + LoRA ONLY"""
    
    def __init__(self, model_path: str, device: str = "cuda", precision: str = "fp16"):
        self.model_path = model_path
        self.device = device
        self.precision = precision
        self.pipeline = None
        self.is_loaded = False
        
        if HUNYUAN_AVAILABLE:
            self._load_pipeline()
        else:
            print("[CRITICAL ERROR] HunyuanWorld not available")
            print("[FAILURE REASON] Cannot load real AI inference components")
            print("[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError("HunyuanWorld not available. Please ensure proper installation.")
    
    def _load_pipeline(self):
        """Load the HunyuanWorld Image2Panorama pipeline - NO FALLBACKS"""
        try:
            print(f"[INFO] Loading HunyuanWorld Image2Panorama pipeline...")
            
            # HunyuanWorld configuration
            lora_path = "tencent/HunyuanWorld-1"
            base_model_path = "black-forest-labs/FLUX.1-fill-dev"
            
            # Create pipeline with proper dtype
            dtype = torch.bfloat16 if self.precision == "bf16" else (
                torch.float16 if self.precision == "fp16" else torch.float32
            )
            
            # Use device_map strategy that HunyuanWorld supports
            device_strategy = "balanced" if "cuda" in self.device else "auto"
            
            self.pipeline = Image2PanoramaPipelines.from_pretrained(
                base_model_path,
                torch_dtype=dtype,
                device_map=device_strategy
            )
            
            # Load HunyuanWorld LoRA
            self.pipeline.load_lora_weights(lora_path)
            
            # Move to device
            self.pipeline = self.pipeline.to(self.device)
            
            self.is_loaded = True
            print(f"[SUCCESS] HunyuanWorld Image2Panorama loaded successfully")
            
        except Exception as e:
            print(f"[CRITICAL ERROR] Failed to load HunyuanWorld Image2Panorama pipeline: {e}")
            print(f"[FAILURE REASON] Real AI inference required but failed")
            print(f"[NO FALLBACK] Procedural generation disabled")
            self.is_loaded = False
            raise RuntimeError(f"HunyuanWorld Image2Panorama pipeline loading failed: {e}")
    
    def generate_panorama(self, image: torch.Tensor, **kwargs):
        """Generate panorama from input image using FLUX + LoRA ONLY"""
        if not self.is_loaded or not HUNYUAN_AVAILABLE:
            print("[CRITICAL ERROR] HunyuanWorld models not loaded")
            print("[FAILURE REASON] Cannot generate without real AI inference")
            print("[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError("HunyuanWorld models not loaded. Please ensure proper installation.")
        
        try:
            print(f"[INFO] Converting image to panorama with FLUX + HunyuanWorld LoRA: {image.shape}")
            
            # Convert tensor to PIL Image
            if len(image.shape) == 4:
                image = image[0]  # Remove batch dim
            
            # Convert from tensor to PIL
            import numpy as np
            from PIL import Image as PILImage
            
            if image.max() <= 1.0:
                image_np = (image.cpu().numpy() * 255).astype(np.uint8)
            else:
                image_np = image.cpu().numpy().astype(np.uint8)
            
            pil_image = PILImage.fromarray(image_np)
            
            # Extract parameters
            strength = kwargs.get('strength', 0.8)
            num_inference_steps = kwargs.get('num_inference_steps', 30)
            guidance_scale = kwargs.get('guidance_scale', 7.5)
            
            # Generate panorama using HunyuanWorld
            result = self.pipeline(
                image=pil_image,
                strength=strength,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
            )
            
            # Convert result back to tensor
            if hasattr(result, 'images') and len(result.images) > 0:
                pil_result = result.images[0]
                result_array = np.array(pil_result).astype(np.float32) / 255.0
                tensor = torch.from_numpy(result_array)
                
                print(f"[SUCCESS] Generated panorama: {tensor.shape}")
                return tensor
            else:
                print(f"[CRITICAL ERROR] No image generated by HunyuanWorld pipeline")
                print(f"[FAILURE REASON] Pipeline executed but produced no output")
                raise RuntimeError("No image generated by HunyuanWorld pipeline")
                
        except Exception as e:
            print(f"[CRITICAL ERROR] Image panorama generation failed: {e}")
            print(f"[FAILURE REASON] Real AI inference required but failed")
            print(f"[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError(f"Image panorama generation failed: {e}")
    
    def to(self, device):
        """Move model to device"""
        self.device = device
        if self.pipeline is not None:
            self.pipeline = self.pipeline.to(device)
        return self
    
    def cpu(self):
        """Move model to CPU"""
        return self.to("cpu")

class HunyuanSceneGeneratorModel:
    """Real HunyuanWorld Scene Generation Integration - FLUX + LoRA ONLY"""
    
    def __init__(self, model_path: str, device: str = "cuda", precision: str = "fp16"):
        self.model_path = model_path
        self.device = device
        self.precision = precision
        self.layer_decomposer = None
        self.world_composer = None
        self.is_loaded = False
        
        if HUNYUAN_AVAILABLE:
            self._load_components()
        else:
            print("[CRITICAL ERROR] HunyuanWorld not available")
            print("[FAILURE REASON] Cannot load real AI inference components")
            print("[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError("HunyuanWorld not available. Please ensure proper installation.")
    
    def _load_components(self):
        """Load HunyuanWorld scene generation components - NO FALLBACKS"""
        try:
            print(f"[INFO] Loading HunyuanWorld scene generation components...")
            
            # Load layer decomposition
            self.layer_decomposer = LayerDecomposition()
            
            # Load world composer
            self.world_composer = WorldComposer()
            
            self.is_loaded = True
            print(f"[SUCCESS] HunyuanWorld scene generation loaded successfully")
            
        except Exception as e:
            print(f"[CRITICAL ERROR] Failed to load scene generation: {e}")
            print(f"[FAILURE REASON] Real AI inference required but failed")
            print(f"[NO FALLBACK] Procedural generation disabled")
            self.is_loaded = False
            raise RuntimeError(f"HunyuanWorld scene generation loading failed: {e}")
    
    def generate_scene(self, panorama: torch.Tensor, **kwargs):
        """Generate 3D scene data from panorama using real AI ONLY"""
        if not self.is_loaded or not HUNYUAN_AVAILABLE:
            print("[CRITICAL ERROR] HunyuanWorld scene generation not available")
            print("[FAILURE REASON] Cannot generate without real AI inference")
            print("[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError("HunyuanWorld scene generation not available. Please ensure proper installation.")
        
        try:
            print(f"[INFO] Generating 3D scene from panorama with real AI: {panorama.shape}")
            
            # Convert tensor to format expected by HunyuanWorld
            # Implementation depends on HunyuanWorld's expected input format
            
            # Use layer decomposition
            layers = self.layer_decomposer.decompose(panorama)
            
            # Generate depth and semantic masks
            depth_map = self.world_composer.estimate_depth(panorama)
            semantic_masks = self.world_composer.segment_scene(panorama)
            
            print(f"[SUCCESS] Generated scene with depth: {depth_map.shape}")
            return depth_map, semantic_masks
            
        except Exception as e:
            print(f"[CRITICAL ERROR] Scene generation failed: {e}")
            print(f"[FAILURE REASON] Real AI inference required but failed")
            print(f"[NO FALLBACK] Procedural generation disabled")
            raise RuntimeError(f"Scene generation failed: {e}")

def get_hunyuan_model_class(model_type: str):
    """Factory function to get appropriate model class"""
    if model_type == "text_to_panorama":
        return HunyuanTextToPanoramaModel
    elif model_type == "image_to_panorama":
        return HunyuanImageToPanoramaModel
    elif model_type == "scene_generator":
        return HunyuanSceneGeneratorModel
    else:
        print(f"[CRITICAL ERROR] Unknown model type: {model_type}")
        print(f"[FAILURE REASON] Invalid model type requested")
        print(f"[NO FALLBACK] Only real AI model types supported")
        raise ValueError(f"Unknown model type: {model_type}")

# Export for integration
__all__ = [
    'HunyuanTextToPanoramaModel',
    'HunyuanImageToPanoramaModel', 
    'HunyuanSceneGeneratorModel',
    'get_hunyuan_model_class',
    'HUNYUAN_AVAILABLE'
]